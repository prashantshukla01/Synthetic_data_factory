Metadata-Version: 2.4
Name: genfactory
Version: 0.1.0
Summary: Autonomous Synthetic Data Pipeline and Model Fine-Tuning
Author-email: Prashant Shukla <prashantshukla9812@gmail.com>
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: langchain>=0.1.0
Requires-Dist: langgraph>=0.1.0
Requires-Dist: langchain-google-genai
Requires-Dist: langsmith>=0.1.0
Requires-Dist: python-dotenv
Requires-Dist: redis
Requires-Dist: datasets
Requires-Dist: transformers
Requires-Dist: peft
Requires-Dist: trl
Requires-Dist: accelerate
Requires-Dist: sentence-transformers
Requires-Dist: streamlit
Provides-Extra: laptop
Requires-Dist: llama-cpp-python; extra == "laptop"
Requires-Dist: huggingface-hub[cli]; extra == "laptop"
Provides-Extra: cluster
Requires-Dist: bitsandbytes; extra == "cluster"
Requires-Dist: deepspeed; extra == "cluster"
Requires-Dist: tensorboard; extra == "cluster"

# ðŸš€ GenFactory
### Autonomous Synthetic Data Pipeline & Model Fine-Tuning Framework

GenFactory is an end-to-end, production-grade framework for **synthetic data generation and model distillation** using a **Teacherâ€“Student architecture**.

The system leverages **Gemini (Teacher models)** to autonomously generate and evaluate high-quality, domain-specific datasets via a **self-healing LangGraph orchestration loop**, and then fine-tunes a lightweight **Phi-3 (Student model)** for efficient deployment in niche domains such as **Python automation**, **FinTech security**, and other specialized technical areas.

This project is designed with **industry-grade reliability, observability, and scalability** in mind.

---

## âœ¨ Key Features

### ðŸ§  Autonomous Orchestration
- LangGraph-powered stateful workflow
- Generator â†’ Judge â†’ Retry â†’ Accept execution loop.
- Automatic retry handling and failure recovery

### ðŸ›¡ï¸ Self-Healing Data Quality
- Dedicated **Judge LLM (Gemini 3.0 Flash Preview )**
- Multi-metric evaluation (relevance, correctness, clarity)
- Only samples scoring **â‰¥ 8/10** are accepted
- Controlled retry logic for weak outputs

### ðŸ’¾ Persistent Memory & Crash Safety
- **Redis Stack**â€“based checkpointing (RedisJSON + RediSearch)
- Safe resume for long-running generation jobs
- Durable state storage across batches

### ðŸ§ª Hardware-Aware Fine-Tuning
- Apple Silicon (M3) optimized via **MPS + FP16**
- NVIDIA cluster training via **QLoRA + bf16**
- Separate pipelines for laptop and supercomputer training

### ðŸ“Š Industry-Level Observability
- Full tracing with **LangSmith**
- Inspect every LLM decision and graph transition
- Cost, retry, and quality visibility

---

## ðŸ—ï¸ Technical Architecture

The system is structured into **four industrial layers**, each with a single responsibility:

### Level 1 â€” Orchestration Brain
**LangGraph + Redis Stack**
- Stateful control flow
- Conditional routing and retries
- Crash-safe checkpointing

### Level 2 â€” Data Engineering
**JSONL Sink + Hugging Face Hub**
- Deterministic formatting
- Incremental dataset construction
- Automated versioned publishing

### Level 3 â€” Domain Training
**SFT + LoRA / QLoRA**
- Phi-3 fine-tuning
- Hardware-specific optimization paths

### Level 4 â€” Evaluation Loop
**Accuracy Gain Benchmarking**
- Base vs fine-tuned model comparison
- Promotion gating
- Regression detection

---

## ðŸ“‚ Project Structure

```text
ai-factory-project/
â”œâ”€â”€ .env                         # API keys (Gemini, OpenAI, HF, LangSmith)
â”‚
â”œâ”€â”€ config/                      # Centralized configuration
â”‚   â”œâ”€â”€ settings.yaml            # Thresholds (score >= 8), model names, retries
â”‚   â””â”€â”€ prompts.yaml             # System prompts (Generator, Judge, Formatter)
â”‚
â”œâ”€â”€ data/                        # Local data storage
â”‚   â”œâ”€â”€ raw/                     # Initial seeds / prompts
â”‚   â”œâ”€â”€ processed/               # JSONL sink (Level 2 output)
â”‚   â””â”€â”€ checkpoints/             # Graph + state backups
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                  # Entry point to run the factory
â”‚
â”‚   â”œâ”€â”€ graph/                   # Level 1: Orchestration Brain (LangGraph)
â”‚   â”‚   â”œâ”€â”€ state.py              # State schema (retry_count, score, trace_id)
â”‚   â”‚   â”œâ”€â”€ nodes.py              # Generator, Judge, Formatter
â”‚   â”‚   â”œâ”€â”€ edges.py              # Conditional routing (score < 8 vs >= 8)
â”‚   â”‚   â””â”€â”€ workflow.py           # Graph compilation + Redis checkpointing
â”‚
â”‚   â”œâ”€â”€ data_eng/                # Level 2: Data Engineering Layer
â”‚   â”‚   â”œâ”€â”€ formatter.py          # JSONL / ChatML normalization
â”‚   â”‚   â””â”€â”€ hf_uploader.py        # HuggingFace dataset syncing
â”‚
â”‚   â”œâ”€â”€ evaluation/              # Level 4: Evaluation & Feedback Loop
â”‚   â”‚   â”œâ”€â”€ benchmarks.py         # Regression & drift detection
â”‚   â”‚   â””â”€â”€ metrics.py            # Accuracy, hallucination, score tracking
â”‚
â”‚   â”œâ”€â”€ training/                # Level 3: Fine-tuning pipelines
â”‚   â”‚   â”œâ”€â”€ train_laptop.py       # Apple Silicon (MPS, FP16)
â”‚   â”‚   â””â”€â”€ train_super.py        # A100 (CUDA, QLoRA, bf16)
â”‚
â”‚   â””â”€â”€ utils/                   # Shared utilities
â”‚       â”œâ”€â”€ database.py           # Redis / Postgres connections
â”‚       â””â”€â”€ logger.py             # LangSmith / custom logging
â”‚
â”œâ”€â”€ notebooks/                   # Exploration & analysis
â”‚   â””â”€â”€ drift_analysis.ipynb      # Score drift visualization
â”‚
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ submit_job.sh                # SLURM submission (HPC runs)

## ðŸš€ Running the App

To launch the Streamlit dashboard, simply run:

```bash
./run_app.sh
```

Alternatively, you can run it manually:

```bash
export PYTHONPATH=$PYTHONPATH:.
streamlit run src/ui/app.py
```
